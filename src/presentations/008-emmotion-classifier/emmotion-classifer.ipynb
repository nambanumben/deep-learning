{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b928a09d-5037-4a82-b859-23a1cd41efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import sounddevice as sd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define emotion labels based on RAVDESS\n",
    "emotion_labels = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c4cb0f2-61a8-4322-913b-10ab9fe1db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess RAVDESS dataset\n",
    "def load_data(dataset_path):\n",
    "    features, labels = [], []\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Iterate through actor folders\n",
    "    for actor_folder in dataset_path.glob('Actor_*'):\n",
    "        for file in actor_folder.glob('*.wav'):\n",
    "            # Extract emotion label from filename (e.g., 03-01-03-01-01-01-01.wav)\n",
    "            emotion = file.name.split('-')[2]\n",
    "            if emotion in emotion_labels:\n",
    "                # Load audio file\n",
    "                y, sr = librosa.load(file, sr=22050)\n",
    "                # Extract MFCC features\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "                # Standardize input length (pad or truncate)\n",
    "                max_len = 100  # Adjust based on your needs\n",
    "                if mfcc.shape[1] < max_len:\n",
    "                    mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "                else:\n",
    "                    mfcc = mfcc[:, :max_len]\n",
    "                # Reshape for CNN: (n_mfcc, time_steps, 1)\n",
    "                mfcc = mfcc.reshape(mfcc.shape[0], mfcc.shape[1], 1)\n",
    "                features.append(mfcc)\n",
    "                labels.append(int(emotion) - 1)  # Convert to 0-based index\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a2d882-9f63-4077-860c-0907753a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    # Fix typo in loss function name\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f46a9a78-eba7-4a07-bba1-868d601ce6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record live audio\n",
    "def record_audio(duration=3, sr=22050):\n",
    "    print(\"Recording... Speak now.\")\n",
    "    audio = sd.rec(int(duration * sr), samplerate=sr, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    audio = audio.flatten()\n",
    "    return audio, sr\n",
    "\n",
    "# Preprocess live audio\n",
    "def preprocess_audio(audio, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    max_len = 100  # Match training data\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    # Reshape for CNN: (1, n_mfcc, time_steps, 1)\n",
    "    mfcc = mfcc.reshape(1, mfcc.shape[0], mfcc.shape[1], 1)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc1bacd-f0e6-432c-a147-abfe185815bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1440, 40, 100, 1), Labels shape: (1440,)\n",
      "Epoch 1/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.1538 - loss: 4.5755 - val_accuracy: 0.1250 - val_loss: 1.9768\n",
      "Epoch 2/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.1949 - loss: 1.9962 - val_accuracy: 0.1875 - val_loss: 1.9528\n",
      "Epoch 3/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.2259 - loss: 1.9781 - val_accuracy: 0.2604 - val_loss: 1.9018\n",
      "Epoch 4/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.2124 - loss: 1.9231 - val_accuracy: 0.2361 - val_loss: 1.8905\n",
      "Epoch 5/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2113 - loss: 1.9590 - val_accuracy: 0.2535 - val_loss: 1.8525\n",
      "Epoch 6/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2132 - loss: 1.8988 - val_accuracy: 0.2292 - val_loss: 1.8004\n",
      "Epoch 7/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.2097 - loss: 1.9145 - val_accuracy: 0.2674 - val_loss: 1.8024\n",
      "Epoch 8/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.2142 - loss: 1.8750 - val_accuracy: 0.2604 - val_loss: 1.7842\n",
      "Epoch 9/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2331 - loss: 1.8474 - val_accuracy: 0.2812 - val_loss: 1.7465\n",
      "Epoch 10/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.2639 - loss: 1.8092 - val_accuracy: 0.2222 - val_loss: 1.7934\n",
      "Epoch 11/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.2084 - loss: 1.8259 - val_accuracy: 0.2639 - val_loss: 1.8302\n",
      "Epoch 12/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.8250 - val_accuracy: 0.2778 - val_loss: 1.7464\n",
      "Epoch 13/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.2509 - loss: 1.7931 - val_accuracy: 0.2917 - val_loss: 1.7343\n",
      "Epoch 14/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.2704 - loss: 1.7290 - val_accuracy: 0.2986 - val_loss: 1.7361\n",
      "Epoch 15/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.3086 - loss: 1.6989 - val_accuracy: 0.3576 - val_loss: 1.6940\n",
      "Epoch 16/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.2992 - loss: 1.6718 - val_accuracy: 0.3125 - val_loss: 1.6040\n",
      "Epoch 17/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.2898 - loss: 1.6551 - val_accuracy: 0.3924 - val_loss: 1.6246\n",
      "Epoch 18/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.3162 - loss: 1.5879 - val_accuracy: 0.3715 - val_loss: 1.5986\n",
      "Epoch 19/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.3603 - loss: 1.5389 - val_accuracy: 0.3264 - val_loss: 1.6274\n",
      "Epoch 20/20\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.3478 - loss: 1.5880 - val_accuracy: 0.3576 - val_loss: 1.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak now.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Detected emotion: sad\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset_path = './presentations/008-emmotion-classifier/dataset'  # Path to your RAVDESS dataset folder\n",
    "    X, y = load_data(dataset_path)\n",
    "    \n",
    "    # Check data shapes\n",
    "    print(f\"Features shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    input_shape = (X.shape[1], X.shape[2], 1)  # (n_mfcc, time_steps, channels)\n",
    "    num_classes = len(emotion_labels)\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # Save model (optional)\n",
    "    model.save('emotion_recognition_cnn_model.h5')\n",
    "    \n",
    "    # Record and classify live audio\n",
    "    audio, sr = record_audio(duration=3)\n",
    "    features = preprocess_audio(audio, sr)\n",
    "    prediction = model.predict(features)\n",
    "    emotion_idx = np.argmax(prediction, axis=1)[0]\n",
    "    print(f\"Detected emotion: {emotion_labels[f'{emotion_idx + 1:02d}']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c1ca24-4b90-423d-bed8-12aeea3f16db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_tkinter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtkinter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageTk\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/tkinter/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_tkinter\u001b[39;00m \u001b[38;5;66;03m# If this fails your Python may not be configured for Tk\u001b[39;00m\n\u001b[1;32m     38\u001b[0m TclError \u001b[38;5;241m=\u001b[39m _tkinter\u001b[38;5;241m.\u001b[39mTclError\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtkinter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_tkinter'"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "def create_gui(model, emotion_labels):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Voice Emotion Recognizer\")\n",
    "    root.geometry(\"400x300\")\n",
    "\n",
    "    # Label to display emotion\n",
    "    emotion_var = tk.StringVar(value=\"Press Record to start\")\n",
    "    emotion_label = tk.Label(root, textvariable=emotion_var, font=(\"Arial\", 16))\n",
    "    emotion_label.pack(pady=20)\n",
    "\n",
    "    # Image display for emotion icon\n",
    "    image_label = tk.Label(root)\n",
    "    image_label.pack(pady=10)\n",
    "\n",
    "    def record_and_predict():\n",
    "        audio, sr = record_audio(duration=5)\n",
    "        features = preprocess_audio(audio, sr)\n",
    "        prediction = model.predict(features)\n",
    "        emotion_idx = np.argmax(prediction, axis=1)[0]\n",
    "        emotion = emotion_labels[f'{emotion_idx + 1:02d}']\n",
    "        emotion_var.set(f\"Detected: {emotion}\")\n",
    "\n",
    "        # Update icon (assumes you have emotion icons in ./icons/)\n",
    "        icon_path = f\"./icons/{emotion}.png\"\n",
    "        if os.path.exists(icon_path):\n",
    "            img = Image.open(icon_path).resize((100, 100))\n",
    "            photo = ImageTk.PhotoImage(img)\n",
    "            image_label.config(image=photo)\n",
    "            image_label.image = photo\n",
    "\n",
    "    # Record button\n",
    "    record_button = tk.Button(root, text=\"Record\", command=record_and_predict, font=(\"Arial\", 14))\n",
    "    record_button.pack(pady=20)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "# Call GUI after training\n",
    "create_gui(model, emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdda58-fc0e-4ddd-9454-de4616fa8b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
